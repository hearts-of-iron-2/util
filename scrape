#!/usr/bin/env python


import requests
import json
from bs4 import BeautifulSoup

base_url = "https://hoi2.paradoxwikis.com"

result = {"articles": []}

stash = {"a": []}


def extract_page_info(soup):
    main = soup.select("main")[0]
    title = main.select("#firstHeading")[0].text
    c = main.select(".mw-parser-output")[0]
    for e in c.select(".thumb"):
        e.decompose()
    el = c.select("#External_link")
    if el:
        el = el[0]
        for e in el.find_all_next():
            e.decompose()
        el.decompose()
    stash["a"].append({"title": title, "content": c.prettify()})
    return {
        "title": title,
        "html": f"<h1>{title}</h1><div class='article'>{c.prettify()}</div>",
    }


def get_all_page_urls(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    url_list = soup.select(".mw-allpages-chunk")[0]
    urls = url_list.find_all("a", href=True)
    return [f"{base_url}{a['href']}" for a in urls]


for url in get_all_page_urls("https://hoi2.paradoxwikis.com/Special:AllPages"):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    info = extract_page_info(soup)
    result["articles"].append(info)


for url in get_all_page_urls(
    "https://hoi2.paradoxwikis.com/index.php?title=Special:AllPages&from=Natural+Rubber+Production+per+country+1936-1948"
):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    info = extract_page_info(soup)
    result["articles"].append(info)

with open("hoi.json", "w") as o:
    print(rf"{json.dumps(result)}", file=o)

with open("stash.json", "w") as o:
    print(rf"{json.dumps(stash)}", file=o)


# response = requests.get("https://hoi2.paradoxwikis.com/1914")
# soup = BeautifulSoup(response.content, "html.parser")
# print(rf"{extract_page_info(soup)['text']}")
